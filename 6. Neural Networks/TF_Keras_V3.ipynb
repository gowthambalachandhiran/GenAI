{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and use it\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant creates constant values, these values do not change\n",
    "or update during optimization process(training phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([50,10])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things that we can do with the tensors at the runtime:\n",
    "1. Directly get a numpy value of the tensor\n",
    "2. dtype : data type of the tensor(int16/int32/float32/float64)\n",
    "3. shape: shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a in tensorflow ==>', a)\n",
    "print('numpy value of a ==>', a.numpy())\n",
    "print('dtype of a ==>', a.dtype)\n",
    "print('shape of a ==>', a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use inbuild tf.XX() function to create constant tensors, just like numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tensor of Ones: \\n',tf.ones(shape=(2, 2)))\n",
    "print('Tensor of Zeros: \\n',tf.zeros(shape=(2, 2)))\n",
    "print('Random normal values \\n', tf.random.normal(shape=(3, 2),\n",
    "                                                  mean=5, \n",
    "                                                  stddev=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generally create a variable with some values, call it initialized values, \n",
    "convert this constant tensor into a variable and then mutate the variable by using special functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5) # Simple variable\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialized variable, like we need for our weights\n",
    "w = tf.Variable(tf.random.normal(shape=(2, 2))) \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(5) # Simple variable\n",
    "print(m)\n",
    "\n",
    "m = tf.Variable(5) \n",
    "print('New value', m.assign(2))\n",
    "\n",
    "m = tf.Variable(5) \n",
    "print('increment by 1', m.assign_add(1))\n",
    "\n",
    "m = tf.Variable(5) \n",
    "print('Decrement by 2', m.assign_sub(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model building in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This step is for data creation, x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step is for data creation, x and y\n",
    "import numpy as np\n",
    "x_train= np.array(range(5000,5100)).reshape(-1,1)\n",
    "\n",
    "\n",
    "y_train=[3*i+np.random.normal(500, 10) for i in x_train]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"x_train vs y_train data\")\n",
    "plt.plot(x_train, y_train, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model y=X*W + b\n",
    "#Model function\n",
    "def output(x):\n",
    "    return W*x + b\n",
    "\n",
    "#Loss function Reduce mean square\n",
    "def loss_function(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "#Initialize Weights\n",
    "W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n",
    "b = tf.Variable(tf.ones(shape=(1,)))\n",
    "\n",
    "#Optimization\n",
    "## Writing training/learing loop with GradienTape\n",
    "learning_rate = 0.000000001\n",
    "steps = 200 #epochs\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = output(x_train)\n",
    "        loss = loss_function(predictions,y_train)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    print(f\"epoch : {i}, loss  {loss.numpy()},  W : {W.numpy()}, b  {b.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w ', W)\n",
    "print('b ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Weights\n",
    "W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n",
    "b = tf.Variable(tf.ones(shape=(1,)))\n",
    "\n",
    "#Optimization\n",
    "## Writing training/learing loop with GradienTape\n",
    "learning_rate = 0.000000001\n",
    "steps = 200 #epochs\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = output(x_train)\n",
    "        loss = loss_function(predictions,y_train)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    if i%30 == 0:\n",
    "        print(f\"epoch is: {i}, loss is {loss.numpy()},  W is: {W.numpy()}, b is {b.numpy()}\")\n",
    "        plt.title([\"epoch\", i])\n",
    "        plt.plot(x_train, y_train, 'b.')\n",
    "        plt.plot(x_train, output(x_train), c='r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model building in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is for data creation\n",
    "x_train= np.random.rand(100,1)\n",
    "y_train=np.array([0 if i < 0.5 else 1 for i in x_train]).reshape(-1,1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"x_train vs y_train data\")\n",
    "plt.plot(x_train, y_train, 'b.',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model y=sigmoid(X*W + b)\n",
    "# same as the linear regression just sigmoid wrapped around the linear equation\n",
    "def output(x): \n",
    "    return tf.sigmoid(W*x + b)\n",
    "\n",
    "#Loss function : sum of squares\n",
    "def loss_function(y_pred, y_true):\n",
    "    return tf.reduce_sum(tf.square(y_pred - y_true))\n",
    "\n",
    "#Initialize Weights\n",
    "W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n",
    "b = tf.Variable(tf.zeros(shape=(1,)))\n",
    "\n",
    "## Optimization\n",
    "learning_rate = 0.1\n",
    "steps = 300 #epochs\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = output(x_train)\n",
    "        loss = loss_function(y_train, predictions)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    print(f\"epoch : {i}, loss  {loss.numpy()},  W : {W.numpy()}, b  {b.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model y=X*W + b\n",
    "# same as the linear regression just sigmoid wrapped around the linear equation\n",
    "def output(x): \n",
    "    return tf.sigmoid(W*x + b)\n",
    "\n",
    "#Loss function : sum of squares\n",
    "def loss_function(y_pred, y_true):\n",
    "    return tf.reduce_sum(tf.square(y_pred - y_true))\n",
    "\n",
    "#Initialize Weights\n",
    "W = tf.Variable(tf.random.uniform(shape=(1, 1)))\n",
    "b = tf.Variable(tf.zeros(shape=(1,)))\n",
    "\n",
    "## Optimization\n",
    "learning_rate = 0.1\n",
    "steps = 300 #epochs\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = output(x_train)\n",
    "        loss = loss_function(y_train, predictions)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "\n",
    "    if i%40 == 0:\n",
    "        print(f\"epoch is: {i}, loss is {loss.numpy()},  W is: {W.numpy()}, b is {b.numpy()}\")\n",
    "        plt.title([\"epoch\", i])\n",
    "        plt.plot(x_train, y_train, 'b+')\n",
    "        plt.plot(x_train, output(x_train), '.', c='r')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data, shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "num_classes=10\n",
    "x_train = X_train.reshape(60000, 784)\n",
    "x_test = X_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "## Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "print(x_train.shape, 'train input samples')\n",
    "print(x_test.shape, 'test input samples')\n",
    "\n",
    "print(y_train.shape, 'train output samples')\n",
    "print(y_test.shape, 'test output samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4 images as gray scale\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[6], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[7], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[9], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Input(shape=(784,)))\n",
    "#Input Layer. The model needs to know what input shape it should expect. \n",
    "#For this reason, the first layer in a Sequential model needs to receive information about its input shape.\n",
    "#Only the first need the snape information, because following layers can do automatic shape inference\n",
    "model.add(layers.Dense(20, activation='sigmoid')) #, input_shape=(784,)))\n",
    "\n",
    "#The dense layer is simply a layer where each unit or neuron is connected to each neuron in the next layer.\n",
    "model.add(layers.Dense(20, activation='sigmoid'))\n",
    "\n",
    "#In the final layer mention the output classes\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model : we define loss function, optimizer and validation matric of our choice\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit method: actually running our model by supplying our input and validation data\n",
    "model.fit(x_train, y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(\"Test Accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
